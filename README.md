# arith25-stochastic-rounding
Support materials for "On Stochastic Rounding with Few Random Bits", Fitzgibbon and Felix, ARITH 2025

This repository is a fork of [nanoGPT](https://github.com/karpathy/nanoGPT) with changes in order to enable quantization-aware training with various float formats and definitions of stochastic rounding from the [gfloat](https://github.com/graphcore-research/gfloat) library.

The original nanoGPT readme is in !(README-original).
